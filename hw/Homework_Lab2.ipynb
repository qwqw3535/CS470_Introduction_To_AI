{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS470 - Introduction to Artificial Intelligence\n",
        "#### Prof. Ho-Jin Choi\n",
        "#### School of Computing, KAIST\n",
        "---"
      ],
      "metadata": {
        "id": "sg8x3G2bkRjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information\n",
        "**Please write your student ID and name here!**\n",
        "- Student ID:\n",
        "- Name:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9b1N0HD4kRjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework #2: Action Recognition\n",
        "\n",
        "In this assignment, you are going to recognize an action of a given video using both a convolutional neural network and a recurrent neural network. Please follow the steps below to continue this assignment."
      ],
      "metadata": {
        "id": "RdbHXV3-kRjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guidelines\n",
        "Copy this example to your Google Colab and edit it to complete your assignment. We should be able to reproduce your results using your code. Please double-check if your code runs without error properly. Submissions failed to run or reproduce the results will get a substantial penalty.\n",
        "\n",
        "### Extra points\n",
        "TAs will rank the submissions based on the test accuracy and assign extra points according to the rank. (The baseline accuracy is roughly 44%.)\n",
        " \n",
        "If you improve your model's accuracy using various techniques (e.g., more stacking layer, early stopping, etc.), then please write how to improve into the block at the bottom. \n",
        "\n",
        "### Deliverables\n",
        "\n",
        "- Download your Colab notebook, and submit a zip file in a format: studentID_Lab2.zip.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions.\n",
        "\n",
        "### Due date\n",
        "\n",
        "- **23:59:59 May 26th (Tue).**\n",
        "- Late submission is allowed until 23:59:59 May 28th (Thu).\n",
        "- Late submission will be applied 20% penalty."
      ],
      "metadata": {
        "id": "zCld6s7RkRjo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# DO NOT EDIT THE FOLLOWING LINES\n",
        "# THESE LINES ARE FOR REPRODUCIBILITY\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WsoWLKKokRjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the UCF101 dataset\n",
        "\n",
        "In this assignment, you will use the UCF101 which is an action recognition dataset of realistic action videos, collected from YouTube, having 101 action categories. (*Soomro, K., Zamir, A. R., & Shah, M. (2012). UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402.*)\n",
        "\n",
        "![UCF101 Dataset](https://github.com/keai-kaist/CS470-Spring-2022-/blob/main/Lab3/May%2012/images/ucf101.jpg?raw=true)"
      ],
      "metadata": {
        "id": "zb5Q8fkvkRjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The UCF101 dataset consists of 13,320 videos and their labels. Since your computing resource in Google Colab is somewhat limited, TA sampled half of the dataset, limited the length of videos to 64 frames, separated videos into frames and stored them to a single file in advance. \n",
        "\n",
        "Let's download and load this file. **(If the link below is not working, please let TA know to fix the link)**"
      ],
      "metadata": {
        "id": "LSGahM4bkRjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pickle\n",
        "\n",
        "if not os.path.exists('ucf101.pickle'):\n",
        "    !wget -O 'ucf101.pickle' 'https://www.dropbox.com/s/2558ailo46px55j/ucf101.pickle?dl=1'\n",
        "\n",
        "    # If the link above is not working, you can also use the following link but it would be slower than the above.\n",
        "    # !wget -O 'ucf101.pickle' 'http://cs492f.keai.io/ucf101.pickle'\n",
        "    \n",
        "with open('ucf101.pickle', 'rb') as input_file:\n",
        "    dataset = pickle.load(input_file)\n",
        "    \n",
        "num_trains = len(dataset['train'])\n",
        "num_validations = len(dataset['validation'])\n",
        "num_tests = len(dataset['test'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "RVwekMqXkRjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "index_to_label = [\n",
        "    'ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', \n",
        "    'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', \n",
        "    'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', \n",
        "    'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', \n",
        "    'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', \n",
        "    'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', \n",
        "    'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', \n",
        "    'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', \n",
        "    'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', \n",
        "    'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', \n",
        "    'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', \n",
        "    'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', \n",
        "    'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', \n",
        "    'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', \n",
        "    'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', \n",
        "    'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', \n",
        "    'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', \n",
        "    'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', \n",
        "    'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', \n",
        "    'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', \n",
        "    'YoYo', \n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "id": "QyYhbGLLkRjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize what some of these videos and their corresponding labels look like."
      ],
      "metadata": {
        "id": "c2VGV_J2kRjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def plot_frames(frames):\n",
        "    figure = plt.figure(figsize=(frames.shape[1] / 72, frames.shape[2] / 72), dpi=72)\n",
        "    image = plt.figimage(frames[0])\n",
        "    \n",
        "    def animate(step):\n",
        "        image.set_array(frames[step])\n",
        "        return (image, )\n",
        "    \n",
        "    video = FuncAnimation(\n",
        "        figure, animate, \n",
        "        frames=len(frames), interval=33, \n",
        "        repeat_delay=1, repeat=True\n",
        "    ).to_html5_video()\n",
        "    \n",
        "    display(HTML(video))"
      ],
      "outputs": [],
      "metadata": {
        "id": "LvIXIsIakRjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for frames, label in random.sample(dataset['train'], 3):\n",
        "    plot_frames(tf.stack([tf.image.decode_jpeg(frame) for frame in frames]))\n",
        "    print('Label:', index_to_label[label])"
      ],
      "outputs": [],
      "metadata": {
        "id": "aLvSJE88kRjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preprocess the dataset\n",
        "\n",
        "Unlike images and text, video data contains both spatial and temporal information. Therefore, to handle these data, you will use both convolutional neural networks and recurrent neural networks to recognize an action of the videos.\n",
        "\n",
        "First, let's extract meaningful features from video frames using the pre-trained convolutional neural networks."
      ],
      "metadata": {
        "id": "tGXjLqHtkRjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem 1\n",
        "Define a model to extract meaningful features from the given video frame using the pre-trained convolutional neural networks of your choice.\n",
        "\n",
        "This model should output a 1D vector for one given frame. That is,\n",
        "- In: `(1, 256, 256, 3)` → Out: `(1, dimension of features of your choice)`\n",
        "- In: `(5, 256, 256, 3)` → Out: `(5, dimension of features of your choice)`\n",
        "- In: `(number of frames, 256, 256, 3)` → Out: `(number of frames, dimension of features of your choice)`"
      ],
      "metadata": {
        "id": "qhphOIlWkRjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# TODO: Define a model to extract features from the given video frame\n",
        "#       using the pre-trained convolutional neural networks of your choice.\n",
        "\n",
        "### START CODE HERE ###\n",
        "cnn_model = \n",
        "### END CODE HERE ###"
      ],
      "outputs": [],
      "metadata": {
        "id": "trASRPYokRjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Problem 2\n",
        "To extract features from the given frames, TA provided `extract_features()` function. In this function, you need to **preprocess each frame** so it can be fed into the `cnn_model`"
      ],
      "metadata": {
        "id": "jytfwUPTkRjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# This function extracts features from the given frames using the defined cnn_model\n",
        "# - In: frames.shape = (number of frames, 256, 256, 3)\n",
        "# - Out: features.shape = (number of frames, dimension of features of your choice)\n",
        "def extract_features(frames, batch=32):\n",
        "    # TODO: Preprocess each frame so it can be fed into the `cnn_model`\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    features = tf.concat([\n",
        "        cnn_model(frames[index:index + batch])\n",
        "        for index in range(0, frames.shape[0], batch)\n",
        "    ], axis=0)\n",
        "    \n",
        "    if features.shape[0] < max_length:\n",
        "        features = tf.concat([\n",
        "            features,\n",
        "            tf.zeros((max_length - features.shape[0], *features.shape[1:]))\n",
        "        ], axis=0)\n",
        "    \n",
        "    return features.numpy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "M1wxpXlhkRjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can extract features from the video but this task is very time-consuming. Therefore, TA provided `preprocess_dataset()` function which takes a dataset, extracts features from the dataset, stores the features into a file, and loads the features from the file."
      ],
      "metadata": {
        "id": "3LkPrDaLkRjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "max_length = 32 # DO NOT CHANGE THIS NUMBER\n",
        "\n",
        "def decode_frames(frames):\n",
        "    return tf.stack([tf.image.decode_jpeg(frame) for frame in frames])\n",
        "\n",
        "def preprocess_dataset(dataset, filename):\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'rb') as input_file:\n",
        "            return pickle.load(input_file)\n",
        "    else:\n",
        "        tensors = [\n",
        "            (extract_features(decode_frames(frames[:max_length])), label)\n",
        "            for frames, label in tqdm(dataset)\n",
        "        ]\n",
        "        \n",
        "        X, y = zip(*tensors)\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        \n",
        "        with open(filename, 'wb') as output_file:\n",
        "            pickle.dump((X, y), output_file, protocol=4) # protocol=4 supports a byte objects larget than 4GB\n",
        "        \n",
        "        return (X, y)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pZIjuZSwkRjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract features from all the datasets. **Please note that the first run takes about 30 minutes.**"
      ],
      "metadata": {
        "id": "qZCNkjfjkRjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_features, train_labels = preprocess_dataset(dataset['train'], f'train-dataset-{max_length}-{num_trains}.pickle')\n",
        "validation_features, validation_labels = preprocess_dataset(dataset['validation'], f'validation-dataset-{max_length}-{num_validations}.pickle')\n",
        "test_features, test_labels = preprocess_dataset(dataset['test'], f'test-dataset-{max_length}-{num_tests}.pickle')"
      ],
      "outputs": [],
      "metadata": {
        "id": "xAa7d4KdkRjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, combine the features into batches."
      ],
      "metadata": {
        "id": "pYr5ZBu2kRjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "batch_size = 64\n",
        "\n",
        "batch_train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(512).batch(batch_size)\n",
        "batch_validation_dataset = tf.data.Dataset.from_tensor_slices((validation_features, validation_labels)).batch(batch_size)\n",
        "batch_test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q62BnQutkRjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build the model\n",
        "All videos are transformed into 2D tensors via convolutional neural networks. To process these tensors, let's build a recurrent neural network.\n",
        "\n",
        "#### Problem 3\n",
        "Define a recurrent neural network to recognize one of `num_classes` actions from the given video. Becaue all videos have different lengths, your `lstm_model` should take account this into account. To do that, TA added `tf.keras.layers.Masking` layer in advance."
      ],
      "metadata": {
        "id": "fsLMjvrnkRju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_classes = len(index_to_label)\n",
        "\n",
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Masking(mask_value=0.0), # DO NOT REMOVE THIS LAYER\n",
        "\n",
        "    # TODO: Define a recurrent neural network to recognize one of `num_classes` actions from the given video\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "])"
      ],
      "outputs": [],
      "metadata": {
        "id": "4LE7zEZhkRju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, compile your model with appropriate parameters."
      ],
      "metadata": {
        "id": "Ga017quMkRju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# TODO: Compile the model with appropriate parameters\n",
        "### START CODE HERE ###\n",
        "lstm_model.compile(\n",
        "\n",
        ")\n",
        "### END CODE HERE ###"
      ],
      "outputs": [],
      "metadata": {
        "id": "vubJV-UVkRju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train the model\n",
        "#### Problem 4\n",
        "Now, train your `lstm_model` using train and validation dataset at least 10 epochs."
      ],
      "metadata": {
        "id": "O9DD6A9NkRju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# TODO: Train the `lstm_model` using train and validation dataset at least 10 epochs\n",
        "### START CODE HERE ###\n",
        "lstm_model.fit(\n",
        "\n",
        ")\n",
        "### END CODE HERE ###"
      ],
      "outputs": [],
      "metadata": {
        "id": "ITu4nyxhkRju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Evaluate accuracy\n",
        "#### Problem 5\n",
        "Let's evaluate the trained model using test dataset and print the test accuracy of the model. For your information, the accuracy of the model proposed by the authors who published the UCF101 dataset is 43.90%."
      ],
      "metadata": {
        "id": "TtFfGv3PkRjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# TODO: Evaluate the model using test dataset\n",
        "### START CODE HERE ###\n",
        "test_accuracy, test_loss = \n",
        "### END CODE HERE ###\n",
        "\n",
        "print(f'Test accuracy: {test_accuracy:.4f}')\n",
        "print(f'Test loss: {test_loss:.4f}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "e0dJ_bOSkRjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the below cell, you can try to recognize an action of the test videos using your trined `lstm_model`."
      ],
      "metadata": {
        "id": "KdXHH9PskRjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for frames, label in random.sample(dataset['test'], 3):\n",
        "    print('Acutal:', index_to_label[label])\n",
        "    \n",
        "    print('Predicted:')\n",
        "    features = extract_features(decode_frames(frames[:max_length]))\n",
        "    predicted = lstm_model(tf.expand_dims(features, 0))[0]\n",
        "    for confidence, index in zip(*tf.math.top_k(predicted, k=3)):\n",
        "        print(f'- {index_to_label[index]} ({confidence.numpy():.4f})')\n",
        "    \n",
        "    plot_frames(decode_frames(frames))\n",
        "    print()"
      ],
      "outputs": [],
      "metadata": {
        "id": "1osei7k0kRjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. How to improve\n",
        "\n",
        "If you have improved the performance of your model, please fill in this block."
      ],
      "metadata": {
        "id": "A37lWRtDkRjv"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Homework - Lab2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}